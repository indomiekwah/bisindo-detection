{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤Ÿ BISINDO Sign Language Recognition - Training (Fixed)\n",
        "\n",
        "**Perbaikan dari versi sebelumnya:**\n",
        "1. âœ… Split data SEBELUM augmentation\n",
        "2. âœ… Kurangi regularisasi (L2 & Dropout)\n",
        "3. âœ… Disable class weights (data sudah balanced)\n",
        "4. âœ… Augmentation lebih ringan\n",
        "5. âœ… Proper validation set (data asli, bukan augmented)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Mount Google Drive"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, regularizers\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Set random seed\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Data"
      ],
      "metadata": {
        "id": "load_data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SESUAIKAN PATH INI\n",
        "# ============================================\n",
        "DATA_DIR = \"/content/drive/MyDrive/BISINDO/processed\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/BISINDO/models\"\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "X_train_full = np.load(f\"{DATA_DIR}/X_train.npy\")\n",
        "X_test = np.load(f\"{DATA_DIR}/X_test.npy\")\n",
        "y_train_full = np.load(f\"{DATA_DIR}/y_train.npy\")\n",
        "y_test = np.load(f\"{DATA_DIR}/y_test.npy\")\n",
        "\n",
        "with open(f\"{DATA_DIR}/label_encoder.pkl\", 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "print(f\"X_train_full: {X_train_full.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"Classes: {len(label_encoder.classes_)}\")\n",
        "print(f\"Class names: {list(label_encoder.classes_)}\")\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "sequence_length = X_train_full.shape[1]\n",
        "num_features = X_train_full.shape[2]"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Split Data SEBELUM Augmentation (PENTING!)\n",
        "\n",
        "**Ini adalah perbaikan utama!**\n",
        "- Validation set harus berisi data ASLI (tidak augmented)\n",
        "- Augmentation hanya diterapkan ke training set"
      ],
      "metadata": {
        "id": "split_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SPLIT SEBELUM AUGMENTATION\n",
        "# ============================================\n",
        "VAL_SIZE = 0.15  # 15% untuk validation\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full,\n",
        "    test_size=VAL_SIZE,\n",
        "    stratify=y_train_full,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "print(f\"After split:\")\n",
        "print(f\"  X_train (for augmentation): {X_train.shape}\")\n",
        "print(f\"  X_val (original data only): {X_val.shape}\")\n",
        "print(f\"  X_test: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "split_before_aug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Augmentation (Lebih Ringan)"
      ],
      "metadata": {
        "id": "aug_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# AUGMENTATION FUNCTIONS (Lebih ringan)\n",
        "# ============================================\n",
        "\n",
        "def add_noise(X, noise_factor=0.01):\n",
        "    \"\"\"Add Gaussian noise - RINGAN.\"\"\"\n",
        "    noise = np.random.normal(0, noise_factor, X.shape)\n",
        "    return X + noise\n",
        "\n",
        "def time_shift(X, shift_max=2):\n",
        "    \"\"\"Shift sequence in time - RINGAN.\"\"\"\n",
        "    X_shifted = np.zeros_like(X)\n",
        "    for i in range(len(X)):\n",
        "        shift = np.random.randint(-shift_max, shift_max + 1)\n",
        "        if shift > 0:\n",
        "            X_shifted[i, shift:] = X[i, :-shift]\n",
        "            X_shifted[i, :shift] = X[i, 0]  # Repeat first frame\n",
        "        elif shift < 0:\n",
        "            X_shifted[i, :shift] = X[i, -shift:]\n",
        "            X_shifted[i, shift:] = X[i, -1]  # Repeat last frame\n",
        "        else:\n",
        "            X_shifted[i] = X[i]\n",
        "    return X_shifted\n",
        "\n",
        "def scale_landmarks(X, scale_range=(0.95, 1.05)):\n",
        "    \"\"\"Scale landmarks randomly - RINGAN.\"\"\"\n",
        "    scales = np.random.uniform(scale_range[0], scale_range[1], (len(X), 1, 1))\n",
        "    return X * scales"
      ],
      "metadata": {
        "id": "aug_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# APPLY AUGMENTATION (Hanya ke training set!)\n",
        "# ============================================\n",
        "\n",
        "print(\"Augmenting training data...\")\n",
        "\n",
        "# Augmentasi RINGAN - hanya 2x data asli\n",
        "X_train_aug1 = add_noise(X_train, noise_factor=0.01)\n",
        "X_train_aug2 = time_shift(X_train, shift_max=2)\n",
        "\n",
        "# Gabungkan: original + 2 augmented = 3x data\n",
        "X_train_combined = np.concatenate([\n",
        "    X_train,       # Original\n",
        "    X_train_aug1,  # Noise\n",
        "    X_train_aug2,  # Time shift\n",
        "], axis=0)\n",
        "\n",
        "y_train_combined = np.concatenate([\n",
        "    y_train,\n",
        "    y_train,\n",
        "    y_train,\n",
        "], axis=0)\n",
        "\n",
        "# Shuffle\n",
        "shuffle_idx = np.random.permutation(len(X_train_combined))\n",
        "X_train_combined = X_train_combined[shuffle_idx]\n",
        "y_train_combined = y_train_combined[shuffle_idx]\n",
        "\n",
        "print(f\"\\nOriginal training: {len(X_train)}\")\n",
        "print(f\"After augmentation: {len(X_train_combined)} (3x)\")\n",
        "print(f\"Validation (NO augmentation): {len(X_val)}\")"
      ],
      "metadata": {
        "id": "apply_aug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ONE-HOT ENCODE\n",
        "# ============================================\n",
        "\n",
        "y_train_cat = to_categorical(y_train_combined, num_classes)\n",
        "y_val_cat = to_categorical(y_val, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"y_train_cat: {y_train_cat.shape}\")\n",
        "print(f\"y_val_cat: {y_val_cat.shape}\")\n",
        "print(f\"y_test_cat: {y_test_cat.shape}\")"
      ],
      "metadata": {
        "id": "one_hot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Architecture (Regularisasi Dikurangi)"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bilstm_model(sequence_length, num_features, num_classes):\n",
        "    \"\"\"\n",
        "    BiLSTM dengan regularisasi RINGAN.\n",
        "    \n",
        "    Perubahan dari sebelumnya:\n",
        "    - L2 regularization: 0.001 -> 0.0001\n",
        "    - Dropout: 0.4-0.5 -> 0.2-0.3\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(sequence_length, num_features))\n",
        "    \n",
        "    # Batch normalization on input\n",
        "    x = layers.BatchNormalization()(inputs)\n",
        "    \n",
        "    # First BiLSTM layer - KURANGI regularisasi\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(128, return_sequences=True,\n",
        "                    kernel_regularizer=regularizers.l2(0.0001),  # Dikurangi\n",
        "                    recurrent_regularizer=regularizers.l2(0.0001))  # Dikurangi\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Dikurangi dari 0.4\n",
        "    \n",
        "    # Second BiLSTM layer\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(64, return_sequences=True,\n",
        "                    kernel_regularizer=regularizers.l2(0.0001),\n",
        "                    recurrent_regularizer=regularizers.l2(0.0001))\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Dikurangi dari 0.4\n",
        "    \n",
        "    # Third BiLSTM layer\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(32, return_sequences=False,\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Dikurangi dari 0.4\n",
        "    \n",
        "    # Dense layers - KURANGI regularisasi\n",
        "    x = layers.Dense(128, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(0.0001))(x)  # Dikurangi\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)  # Dikurangi dari 0.5\n",
        "    \n",
        "    x = layers.Dense(64, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Dikurangi dari 0.3\n",
        "    \n",
        "    # Output\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_simple_lstm(sequence_length, num_features, num_classes):\n",
        "    \"\"\"\n",
        "    Simple LSTM - untuk baseline atau dataset kecil.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(sequence_length, num_features)),\n",
        "        \n",
        "        layers.BatchNormalization(),\n",
        "        \n",
        "        layers.LSTM(64, return_sequences=True),\n",
        "        layers.Dropout(0.2),\n",
        "        \n",
        "        layers.LSTM(32, return_sequences=False),\n",
        "        layers.Dropout(0.2),\n",
        "        \n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        \n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "model_def"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BUILD MODEL\n",
        "# ============================================\n",
        "\n",
        "# Pilih model:\n",
        "USE_SIMPLE_MODEL = False  # True untuk dataset sangat kecil (<100 samples)\n",
        "\n",
        "if USE_SIMPLE_MODEL:\n",
        "    model = build_simple_lstm(sequence_length, num_features, num_classes)\n",
        "    model_name = \"bisindo_simple_lstm\"\n",
        "else:\n",
        "    model = build_bilstm_model(sequence_length, num_features, num_classes)\n",
        "    model_name = \"bisindo_bilstm_fixed\"\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "build_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Compile & Callbacks"
      ],
      "metadata": {
        "id": "compile_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# COMPILE MODEL\n",
        "# ============================================\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Sedikit lebih tinggi\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# CALLBACKS\n",
        "# ============================================\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    ModelCheckpoint(\n",
        "        filepath=f\"{MODEL_DIR}/{model_name}_best.keras\",\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=7,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Model compiled!\")"
      ],
      "metadata": {
        "id": "compile"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training\n",
        "\n",
        "**Perubahan penting:**\n",
        "- Menggunakan `validation_data` (data asli) bukan `validation_split`\n",
        "- TIDAK menggunakan class_weight (data sudah di-balance)"
      ],
      "metadata": {
        "id": "train_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TRAINING\n",
        "# ============================================\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING CONFIG\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Training samples: {len(X_train_combined)} (augmented)\")\n",
        "print(f\"Validation samples: {len(X_val)} (original, NO augmentation)\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Class weights: DISABLED (data already balanced)\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# TRAINING dengan validation_data (BUKAN validation_split)\n",
        "history = model.fit(\n",
        "    X_train_combined, y_train_cat,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val_cat),  # Data asli untuk validasi!\n",
        "    callbacks=callbacks,\n",
        "    # class_weight=None,  # TIDAK pakai class weights\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Visualisasi Training History"
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PLOT TRAINING HISTORY\n",
        "# ============================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "axes[0].set_title('Model Accuracy', fontsize=14)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "axes[1].set_title('Model Loss', fontsize=14)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{MODEL_DIR}/{model_name}_history.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Check if training is normal\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"\\nFinal Training Accuracy: {final_train_acc*100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
        "\n",
        "if final_train_acc > final_val_acc:\n",
        "    print(\"âœ… Normal: Train > Val (sedikit overfitting adalah normal)\")\n",
        "else:\n",
        "    print(\"âš ï¸ Unusual: Val > Train (mungkin masih ada masalah)\")"
      ],
      "metadata": {
        "id": "plot_history"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Evaluation"
      ],
      "metadata": {
        "id": "eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# EVALUATE ON TEST SET\n",
        "# ============================================\n",
        "\n",
        "print(\"Evaluating on test set...\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TEST RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "evaluate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CLASSIFICATION REPORT\n",
        "# ============================================\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "class_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CONFUSION MATRIX\n",
        "# ============================================\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_\n",
        ")\n",
        "plt.title(f'Confusion Matrix - {model_name}\\nTest Accuracy: {test_accuracy*100:.2f}%', fontsize=14)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{MODEL_DIR}/{model_name}_confusion_matrix.png\", dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "confusion_matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Save Model"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SAVE MODEL & INFO\n",
        "# ============================================\n",
        "\n",
        "# Save final model\n",
        "model.save(f\"{MODEL_DIR}/{model_name}_final.keras\")\n",
        "print(f\"Model saved: {MODEL_DIR}/{model_name}_final.keras\")\n",
        "\n",
        "# Save training info\n",
        "info = {\n",
        "    'model_name': model_name,\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_loss': float(test_loss),\n",
        "    'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
        "    'final_val_accuracy': float(history.history['val_accuracy'][-1]),\n",
        "    'epochs_trained': len(history.history['loss']),\n",
        "    'sequence_length': sequence_length,\n",
        "    'num_features': num_features,\n",
        "    'num_classes': num_classes,\n",
        "    'classes': list(label_encoder.classes_),\n",
        "    'training_samples': len(X_train_combined),\n",
        "    'validation_samples': len(X_val),\n",
        "    'test_samples': len(X_test),\n",
        "    'augmentation': '3x (original + noise + time_shift)',\n",
        "    'fixes_applied': [\n",
        "        'Split before augmentation',\n",
        "        'Reduced L2 regularization (0.001 -> 0.0001)',\n",
        "        'Reduced Dropout (0.4-0.5 -> 0.2-0.3)',\n",
        "        'Disabled class weights',\n",
        "        'Lighter augmentation'\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(f\"{MODEL_DIR}/{model_name}_info.json\", 'w') as f:\n",
        "    json.dump(info, f, indent=2)\n",
        "\n",
        "# Copy label encoder\n",
        "import shutil\n",
        "shutil.copy(f\"{DATA_DIR}/label_encoder.pkl\", f\"{MODEL_DIR}/label_encoder.pkl\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ALL FILES SAVED!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  ðŸ“ {model_name}_final.keras\")\n",
        "print(f\"  ðŸ“ {model_name}_best.keras\")\n",
        "print(f\"  ðŸ“ {model_name}_info.json\")\n",
        "print(f\"  ðŸ“ {model_name}_history.png\")\n",
        "print(f\"  ðŸ“ {model_name}_confusion_matrix.png\")\n",
        "print(f\"  ðŸ“ label_encoder.pkl\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š Checklist Hasil yang Diharapkan\n",
        "\n",
        "Setelah training, pastikan:\n",
        "\n",
        "| Metrik | Normal | Tidak Normal |\n",
        "|--------|--------|---------------|\n",
        "| Train Accuracy | > Val Accuracy | < Val Accuracy |\n",
        "| Train Loss | < Val Loss | > Val Loss |\n",
        "| Gap Train-Val | 5-15% | > 30% (overfitting) |\n",
        "\n",
        "**Contoh hasil yang baik:**\n",
        "```\n",
        "Train Accuracy: 92%\n",
        "Val Accuracy: 88%\n",
        "Test Accuracy: 85%\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”§ Troubleshooting\n",
        "\n",
        "**Jika Val > Train masih terjadi:**\n",
        "1. Kurangi augmentation (hanya noise)\n",
        "2. Gunakan `USE_SIMPLE_MODEL = True`\n",
        "3. Tambah data asli (rekam lebih banyak video)\n",
        "\n",
        "**Jika accuracy rendah (<60%):**\n",
        "1. Cek kualitas landmarks extraction\n",
        "2. Tambah epochs\n",
        "3. Kurangi learning rate ke 0.0001"
      ],
      "metadata": {
        "id": "checklist"
      }
    }
  ]
}
