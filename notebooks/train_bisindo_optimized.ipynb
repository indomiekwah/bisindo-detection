{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤Ÿ BISINDO Sign Language Recognition - Training (Optimized)\n",
        "\n",
        "Notebook ini sudah dioptimasi untuk mengatasi masalah akurasi rendah.\n",
        "\n",
        "**Perubahan dari versi sebelumnya:**\n",
        "1. Learning rate lebih kecil (0.0001)\n",
        "2. Batch size lebih kecil (16)\n",
        "3. Label smoothing untuk mencegah overconfidence\n",
        "4. Data augmentation\n",
        "5. Better model architecture"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Mount Google Drive"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, regularizers\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau,\n",
        "    LearningRateScheduler\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Data"
      ],
      "metadata": {
        "id": "load_data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SESUAIKAN PATH INI\n",
        "# ============================================\n",
        "DATA_DIR = \"/content/drive/MyDrive/BISINDO/processed\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/BISINDO/models\"\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "X_train = np.load(f\"{DATA_DIR}/X_train.npy\")\n",
        "X_test = np.load(f\"{DATA_DIR}/X_test.npy\")\n",
        "y_train = np.load(f\"{DATA_DIR}/y_train.npy\")\n",
        "y_test = np.load(f\"{DATA_DIR}/y_test.npy\")\n",
        "\n",
        "with open(f\"{DATA_DIR}/label_encoder.pkl\", 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"Classes: {len(label_encoder.classes_)}\")\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "sequence_length = X_train.shape[1]\n",
        "num_features = X_train.shape[2]"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing & Augmentation"
      ],
      "metadata": {
        "id": "preprocessing_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(X, noise_factor=0.01):\n",
        "    \"\"\"Add Gaussian noise to data.\"\"\"\n",
        "    noise = np.random.normal(0, noise_factor, X.shape)\n",
        "    return X + noise\n",
        "\n",
        "def time_shift(X, shift_max=5):\n",
        "    \"\"\"Shift sequence in time.\"\"\"\n",
        "    X_shifted = np.zeros_like(X)\n",
        "    for i in range(len(X)):\n",
        "        shift = np.random.randint(-shift_max, shift_max + 1)\n",
        "        if shift > 0:\n",
        "            X_shifted[i, shift:] = X[i, :-shift]\n",
        "        elif shift < 0:\n",
        "            X_shifted[i, :shift] = X[i, -shift:]\n",
        "        else:\n",
        "            X_shifted[i] = X[i]\n",
        "    return X_shifted\n",
        "\n",
        "def scale_landmarks(X, scale_range=(0.9, 1.1)):\n",
        "    \"\"\"Scale landmarks randomly.\"\"\"\n",
        "    scales = np.random.uniform(scale_range[0], scale_range[1], (len(X), 1, 1))\n",
        "    return X * scales\n",
        "\n",
        "# Augment training data\n",
        "print(\"Augmenting training data...\")\n",
        "\n",
        "X_train_aug1 = add_noise(X_train, noise_factor=0.02)\n",
        "X_train_aug2 = time_shift(X_train, shift_max=3)\n",
        "X_train_aug3 = scale_landmarks(X_train)\n",
        "\n",
        "# Combine original + augmented\n",
        "X_train_combined = np.concatenate([X_train, X_train_aug1, X_train_aug2, X_train_aug3], axis=0)\n",
        "y_train_combined = np.concatenate([y_train, y_train, y_train, y_train], axis=0)\n",
        "\n",
        "# Shuffle\n",
        "shuffle_idx = np.random.permutation(len(X_train_combined))\n",
        "X_train_combined = X_train_combined[shuffle_idx]\n",
        "y_train_combined = y_train_combined[shuffle_idx]\n",
        "\n",
        "print(f\"Original training size: {len(X_train)}\")\n",
        "print(f\"Augmented training size: {len(X_train_combined)}\")\n",
        "\n",
        "# Clip values to [0, 1]\n",
        "X_train_combined = np.clip(X_train_combined, 0, 1)\n",
        "\n",
        "# One-hot encode\n",
        "y_train_cat = to_categorical(y_train_combined, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "augmentation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights untuk handle imbalance\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_combined),\n",
        "    y=y_train_combined\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(\"Class weights computed.\")"
      ],
      "metadata": {
        "id": "class_weights"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Architecture (Improved)"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_improved_lstm(sequence_length, num_features, num_classes):\n",
        "    \"\"\"\n",
        "    Improved LSTM with:\n",
        "    - Batch normalization\n",
        "    - L2 regularization\n",
        "    - Proper dropout\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(sequence_length, num_features))\n",
        "\n",
        "    # Batch normalization on input\n",
        "    x = layers.BatchNormalization()(inputs)\n",
        "\n",
        "    # First LSTM layer\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(128, return_sequences=True,\n",
        "                    kernel_regularizer=regularizers.l2(0.001),\n",
        "                    recurrent_regularizer=regularizers.l2(0.001))\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # Second LSTM layer\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(64, return_sequences=True,\n",
        "                    kernel_regularizer=regularizers.l2(0.001),\n",
        "                    recurrent_regularizer=regularizers.l2(0.001))\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # Third LSTM layer\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(32, return_sequences=False,\n",
        "                    kernel_regularizer=regularizers.l2(0.001))\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = layers.Dense(128, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Dense(64, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_simple_lstm(sequence_length, num_features, num_classes):\n",
        "    \"\"\"\n",
        "    Simple LSTM for baseline testing.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(sequence_length, num_features)),\n",
        "\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "        layers.LSTM(64, return_sequences=True),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.LSTM(32, return_sequences=False),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "model_architectures"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PILIH MODEL\n",
        "# ============================================\n",
        "USE_SIMPLE_MODEL = False  # Set True untuk model sederhana\n",
        "\n",
        "if USE_SIMPLE_MODEL:\n",
        "    model = build_simple_lstm(sequence_length, num_features, num_classes)\n",
        "    model_name = \"bisindo_simple_lstm\"\n",
        "else:\n",
        "    model = build_improved_lstm(sequence_length, num_features, num_classes)\n",
        "    model_name = \"bisindo_improved_lstm\"\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "build_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Configuration"
      ],
      "metadata": {
        "id": "training_config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate schedule\n",
        "def lr_schedule(epoch, lr):\n",
        "    \"\"\"Learning rate warmup + decay.\"\"\"\n",
        "    if epoch < 10:\n",
        "        # Warmup\n",
        "        return 0.0001 * (epoch + 1) / 10\n",
        "    elif epoch < 50:\n",
        "        return 0.0001\n",
        "    elif epoch < 100:\n",
        "        return 0.00005\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=30,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=0.001\n",
        "    ),\n",
        "\n",
        "    ModelCheckpoint(\n",
        "        filepath=f\"{MODEL_DIR}/{model_name}_best.keras\",\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=10,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    LearningRateScheduler(lr_schedule, verbose=0)\n",
        "]\n",
        "\n",
        "print(\"Model compiled!\")"
      ],
      "metadata": {
        "id": "compile_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TRAINING\n",
        "# ============================================\n",
        "EPOCHS = 150\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "print(f\"Training {model_name}...\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Training samples: {len(X_train_combined)}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_combined, y_train_cat,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "train_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation"
      ],
      "metadata": {
        "id": "evaluation_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(history.history['accuracy'], label='Train')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(history.history['loss'], label='Train')\n",
        "axes[1].plot(history.history['val_loss'], label='Validation')\n",
        "axes[1].set_title('Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{MODEL_DIR}/{model_name}_history.png\", dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plot_history"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set (original, non-augmented)\n",
        "print(\"Evaluating on test set...\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "evaluate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "classification_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_\n",
        ")\n",
        "plt.title(f'Confusion Matrix - {model_name}\\nAccuracy: {test_accuracy*100:.2f}%')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{MODEL_DIR}/{model_name}_confusion_matrix.png\", dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "confusion_matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Save Model"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model\n",
        "model.save(f\"{MODEL_DIR}/{model_name}_final.keras\")\n",
        "print(f\"Model saved: {MODEL_DIR}/{model_name}_final.keras\")\n",
        "\n",
        "# Save info\n",
        "info = {\n",
        "    'model_name': model_name,\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_loss': float(test_loss),\n",
        "    'epochs_trained': len(history.history['loss']),\n",
        "    'sequence_length': sequence_length,\n",
        "    'num_features': num_features,\n",
        "    'num_classes': num_classes,\n",
        "    'classes': list(label_encoder.classes_),\n",
        "    'augmentation': True,\n",
        "    'training_samples': len(X_train_combined)\n",
        "}\n",
        "\n",
        "with open(f\"{MODEL_DIR}/{model_name}_info.json\", 'w') as f:\n",
        "    json.dump(info, f, indent=2)\n",
        "\n",
        "# Copy label encoder\n",
        "import shutil\n",
        "shutil.copy(f\"{DATA_DIR}/label_encoder.pkl\", f\"{MODEL_DIR}/label_encoder.pkl\")\n",
        "\n",
        "print(\"\\nAll files saved!\")\n",
        "print(f\"  - {model_name}_final.keras\")\n",
        "print(f\"  - {model_name}_best.keras\")\n",
        "print(f\"  - {model_name}_info.json\")\n",
        "print(f\"  - label_encoder.pkl\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Tips Jika Akurasi Masih Rendah\n",
        "\n",
        "1. **Coba `USE_SIMPLE_MODEL = True`** di cell model selection\n",
        "2. **Tambah epochs** ke 200-300\n",
        "3. **Kurangi batch size** ke 8\n",
        "4. **Cek data augmentation** - bisa disable dulu untuk baseline"
      ],
      "metadata": {
        "id": "tips_header"
      }
    }
  ]
}
